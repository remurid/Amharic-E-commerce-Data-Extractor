{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54823263",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# task 3: Fine-Tuning the Model\n",
    "# ==============================================================================\n",
    "# Now we bring everything together to train the model.\n",
    "\n",
    "# --- Data Collator ---\n",
    "# This helper object creates batches of data for training. It will also\n",
    "# pad our sentences to be the same length within a batch.\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "# This function calculates the performance of our model during evaluation.\n",
    "# It computes precision, recall, and F1-score, as required by the assignment.\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (-100) and convert predictions to label strings\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# --- Calculate Class Weights ---\n",
    "# To address class imbalance, we calculate weights based on label frequencies.\n",
    "# Less frequent labels will have higher weights.\n",
    "if \"train\" in tokenized_datasets:\n",
    "    label_counts = {}\n",
    "    for example in tokenized_datasets[\"train\"]:\n",
    "        for label_id in example[\"labels\"]:\n",
    "            if label_id != -100: # Ignore padding/special tokens\n",
    "                label_counts[label_id] = label_counts.get(label_id, 0) + 1\n",
    "\n",
    "    total_labels = sum(label_counts.values())\n",
    "    # Ensure all possible label IDs from 0 to len(id2label)-1 are included\n",
    "    # even if they don't appear in the training data (assign a small count or handle carefully)\n",
    "    # For simplicity, we'll use counts from the training data, assuming all relevant\n",
    "    # labels appear at least once. If not, you might need to adjust this.\n",
    "    num_classes = len(id2label)\n",
    "    # Initialize counts for all classes to avoid division by zero if a class is missing\n",
    "    full_label_counts = {i: label_counts.get(i, 0) for i in range(num_classes)}\n",
    "\n",
    "    # Calculate inverse frequency weights\n",
    "    # weight_i = total_labels / (num_classes * count_i) or similar\n",
    "    # A common approach is 1 / frequency, then normalize.\n",
    "    # Using total samples / (num_classes * count) helps scale.\n",
    "    weights = [0.0] * num_classes\n",
    "    for i in range(num_classes):\n",
    "        count = full_label_counts[i]\n",
    "        # Add a small smoothing term to avoid division by zero for unseen labels\n",
    "        weights[i] = total_labels / (num_classes * (count + 1e-5))\n",
    "\n",
    "    import torch\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "    print(\"\\nCalculated class weights:\")\n",
    "    print(class_weights)\n",
    "else:\n",
    "    class_weights = None\n",
    "    print(\"\\nSkipping class weight calculation as training data is not available.\")\n",
    "\n",
    "# --- Load the Pre-trained Model ---\n",
    "# We load the XLM-Roberta model but tell it we are using it for \"Token\n",
    "# Classification\". We also pass our label mappings so it knows what to predict.\n",
    "# We'll pass the calculated class weights to the model's configuration.\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "# Pass class weights to the configuration if they were calculated\n",
    "if class_weights is not None:\n",
    "    # Convert the tensor to a list for JSON serialization\n",
    "    config.class_weights = class_weights.tolist() # Convert tensor to list\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    config=config, # Pass the modified configuration\n",
    ")\n",
    "print(f\"\\nModel '{model_checkpoint}' loaded and configured for NER with class weights.\")\n",
    "\n",
    "\n",
    "# --- Define Training Arguments ---\n",
    "# These are the settings for our training process (hyperparameters).\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"amharic-ner-model\", # Directory to save the final model\n",
    "    learning_rate=2e-5,            # A standard starting learning rate\n",
    "    per_device_train_batch_size=8, # How many sentences to process at once\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,            # Increased epochs to potentially help with imbalance\n",
    "    weight_decay=0.01,             # Helps prevent the model from overfitting\n",
    "    eval_strategy=\"epoch\",   # Run evaluation at the end of each epoch\n",
    "    save_strategy=\"epoch\",         # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,   # Load the best performing checkpoint at the end\n",
    "    metric_for_best_model=\"f1\",    # Monitor F1 for best model\n",
    "    push_to_hub=False,             # We won't upload to Hugging Face Hub for now\n",
    "    report_to=\"none\", # Disable reporting to services like wandb by default\n",
    ")\n",
    "\n",
    "\n",
    "# --- Initialize the Trainer ---\n",
    "# The Trainer object orchestrates the entire fine-tuning process.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"] if \"test\" in tokenized_datasets else None, # Pass eval_dataset only if it exists\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics, # Our custom evaluation function\n",
    ")\n",
    "\n",
    "print(\"\\nTrainer is initialized. Starting the training process...\")\n",
    "print(\"This may take some time depending on your data size and epochs.\")\n",
    "\n",
    "# This single line starts the entire training process!\n",
    "# You will see a progress bar and the performance metrics after each epoch.\n",
    "trainer.train()\n",
    "\n",
    "# --- Save the Final Model ---\n",
    "# After training, we save the best version of our model to the output directory.\n",
    "# This folder will contain everything needed to use the model later.\n",
    "if training_args.load_best_model_at_end:\n",
    "    print(f\"\\nBest model loaded from checkpoint based on '{training_args.metric_for_best_model}'.\")\n",
    "final_model_path = \"./final-amharic-ner-model\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"\\nTraining complete! Your fine-tuned model is saved in: {final_model_path}\")\n",
    "print(\"You can now download this folder from the Colab file browser to use it later.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
